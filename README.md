# pre_observation_reward_poisoning

## READ THIS FIRST
This repository does not include trained models and raw results due to the size of these files. These files are necessary to run the test codes and re/generate results, but were not included here due to large file size. To replicate the results in the report you will need to either:

  1 - Run the the training and experiments from sctratch. However, this is not recommended because due to the large number of training seeds we test (10 per   parameter/algorithm setting 16 algorithms) the entire training takes about **8 hours** on my GPU. The getting the raw results from the experiments also take around as long.
  
  2 - Download the models and raw results from the following box link and place them in the root folder of this project tree.
      https://utexas.box.com/s/srv63hoowgmw9li4jal77kwvofzy4230 

## About
The *reward poisoning attack* is a type of data poisoning attack in which an \textit{attacker} tries to force an *victim* learning agent to learn a predefined, often malicious, *target policy* by perturbing the rewards observed by the victim at training time. These attacks can be classified as either online or offline. In the offline case, the attacker generates a sequence of reward perturbations before training time of the victim algorithm. Then these perturbations sequentially get added to the rewards generated by the environment. In the offline setting, the attacker makes all of its decisions before the training time and cannot interact with the victim agent after the training starts. In contrast, the online reward poisoning attack setting allows the agent to sequentially generate perturbations observed on the rewards generated by the environment during the training time of the victim algorithm.

In this repository we implement RL based reward poisoning attacks on stochastic bandits using three different state of the art RL methods A2C, DDPG, SAC. The difference between this implementation and the existing methods is that we do not assume that the attacker can immediately eavesdrop on the arm choice of the agent before choosing its perturbation. Instead we relax this assumption by letting attacker choose the perturbation *before* it observed which arm the victim agent chooses. This is a significantly more difficult problem than the existing poisoning problems since the attacker cannot know to which arm its perturbation will be added. Thus it has to learn to predict the next arm choice of the victim agent.

## How to run these files?
To run these python files you will need the following dependencies:
- python 3.7.13 (These files might not run on newer versions of python!)
- gym 0.21.0 : https://www.gymlibrary.ml/
- stable_baselines3 1.5.0: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html
- tqdm 4.64.0
- pytorch 1.11.0
- numpy 1.21.6
- pandas 1.3.5
- matplotlib 3.5.1

As mentioned bove in the "READ THIS FIRST" section, there are two ways to replicate our results

### Option 1 - Training from scratch (Not Recommended)
Run the following command to generate trained models. This can take several hours. At the end you should end up with a folder named "models" in the root folder with trained models in it.
```
$ python train_models.py
```

Next, run the following command to test the trained models and obtain raw results. These results are zipped array collections containing the regret of the victim, rewards of the attacker and arm choices of the victim. Running these tests can also take several hours since there are 160 different trained models to test. In the end you should and up with a folder named raw_results in the root folder.
```
$ python test_models.py
```

Lastly run the following to get the result summary table and the plots. These results should be available in the refined_results folder. You are welcome compare them with the refined_results we include in this repository.
```
$ python prettify_results.py
```

### Option 2 - Download trained models
Go to https://utexas.box.com/s/srv63hoowgmw9li4jal77kwvofzy4230 to download the trained models and raw rewards (should be around 1 GB). Include them in the root folder. Then you can directly run:

```
$ python prettify_results.py
```

Again, you are welcome compare them with the refined_results we include in this repository.

